# HW4 George W Nakhla


Package installs:
```{r}
#install.packages("MPV")
library("MPV")
library("knitr")
#install.packages("ggplot2")
library("ggplot2")
#install.packages("qpcR")
library(qpcR)
#install.packages("dplyr")
library(dplyr)
library(lme4)
library(Matrix)
```


## 1. Question 5.5. A glass bottle manufacturing company has recorded data on the average number of defects per 10,000 bottles due to stones (small pieces of rock embedded in the bottle wall) and the number of weeks since the last furnace overhaul.

```{r}
# create df from supplied data

defects_per_10000 <- c(13.0, 16.1, 14.5, 17.8, 22.0, 27.4, 16.8, 34.2, 65.6, 49.2, 66.2, 81.2, 87.4, 114.5)
weeks <- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17)
glass_defects <- data.frame(defects_per_10000, weeks)
```

### a. Fit a straight-line regression model to the data and perform the standard tests for model adequacy.
```{r}
glassFit <- lm(defects_per_10000 ~ weeks, data = glass_defects)

ggplot(glass_defects, aes(x = weeks, y = defects_per_10000)) +
  geom_point() +  # Add points
  geom_smooth(method = "lm", se = FALSE) +  # Add linear regression line
  labs(x = "Weeks", y = "Defects per 10,000") +  # Add axis labels
  ggtitle("Linear Regression of Defects per 10,000 over Weeks")


par(mfrow=c(2,2))  # Set up the plotting layout
plot(glassFit)

anova(glassFit)
summary(glassFit)
glassFit
```
We see here that there is a positive trend, as weeks increase, so do the number of defects per 10k. Our B0 is -31.698, indicating that at week 0, there are -31.698 defects. This doesnt really make sense in a case like this, defects can only be 0 per 10K, so we can conclude that it is negative because the defects are less frequent than 10K, i.e. they still occur, but maybe more likely in say, 20K bottles. Out B1 is 7.277, meaning that for each passing week, we can expect the number of defects to increase by 7.277 defects/10K bottles.

Looking at the significance, we see that we have a P value of 2.35e-06, extremely low for any value of alpha. We can conclude that the model is significant. However, looking at the plot of residuals vs fitted values, we see that they are parabolic in shape, suggesting an issue with our model. We can remedy this through transforms in the next question. 


### b. Suggest an appropriate transformation to eliminate the problems encountered in part a. Fit the transformed model and check for adequacy.
```{r}
log_defects <- log(defects_per_10000)

logGlassFit <- lm(log_defects ~ weeks, data = glass_defects)

par(mfrow=c(2,2))  # Set up the plotting layout
plot(logGlassFit)


```


Applying a log transform improves the shape of our residuals vs Fitted graph and gives us a more scattered shape, although it is not perfect.


## 2. Question 5.12. Vining and Myers (“Combining Taguchi and Response Surface Philosophies:
A dual Response Approach,” Journal of Quality Technology, 22, 15-22, 1990) analyze an
experiment, which originally appeared in Box and Draper [1987]. This experiment studied
the effect of speed ( x1 ), pressure ( x2 ), and distance ( x3 ) on a printing machine’s ability
to apply coloring inks on package labels. The following table summarizes the experimental
results.
## a. Fit an appropriate model to each response and conduct the residual analysis.
```{r}
printerData <- p5.12

# Fit models for each response variable
model_yi1 <- lm(yi1 ~ xi + x2 + x3, data = printerData)
model_yi2 <- lm(yi2 ~ xi + x2 + x3, data = printerData)
model_yi3 <- lm(yi3 ~ xi + x2 + x3, data = printerData)

model_ybar <- lm(ybari ~ xi + x2 + x3, data = printerData)

par(mfrow=c(2,2))  # Set up the plotting layout
plot(model_yi1)
plot(model_yi2)
plot(model_yi3)
plot(model_ybar)
```

We see several issues across the the 3 models, especially with influential point. For model all models we see that the scale location and residuals vs fitted graph is cone/funnel shaped, meaning we don't have homoskedastic errors.
<br>
Looking specifically at cooks distance, we see that in:
Model 1: Points 19, 24, 25 have high impact/leverage on the model
<br>
Model 2: Points 9, 25, 27 have high impact/leverage on the model
<br>
Model 3: Points 9, 25, 27 have high impact/leverage on the model
<br>


## b. Use the sample variances as the basis for weighted least-squares estimation of the original data (not the sample means).
```{r}
# Add a small constant to avoid division by zero
epsilon <- 1e-10  # You can adjust the value of epsilon as needed

# Compute weights with a safeguard against division by zero
weights <- 1 / (printerData$si^2 + epsilon)

# Perform weighted least-squares estimation for each response variable
modelW_yi1 <- lm(yi1 ~ xi + x2 + x3, data = printerData, weights = weights)
modelW_yi2 <- lm(yi2 ~ xi + x2 + x3, data = printerData, weights = weights)
modelW_yi3 <- lm(yi3 ~ xi + x2 + x3, data = printerData, weights = weights)

par(mfrow=c(2,2))  # Set up the plotting layout
plot(modelW_yi1)
plot(modelW_yi2)
plot(modelW_yi3)





```
We see improvement across the board, but there is still a funnel shape within the residuals vs fitted. we can add a transform in the next step to improve the model.


## c. Vining and Myers suggest fitting a linear model to an appropriate transformation of the sample variances. Use such a model to develop the appropriate weights and repeat part b
```{r}
# Define the degree of the polynomial transformation
degree <- 2  # You can adjust this as needed

# Perform polynomial transformation on the sample variances
printerData$poly_si <- printerData$si^degree

# Fit a linear model to the transformed sample variances
variance_model <- lm(poly_si ~ xi + x2 + x3, data = printerData)

# Extract the coefficients from the model
variance_coeffs <- coef(variance_model)

# Use the model to predict the transformed variances
predicted_poly_variances <- predict(variance_model)

# Transform back to obtain the predicted variances
predicted_variances <- predicted_poly_variances^(1/degree)

# Compute weights based on the predicted variances
weights <- 1 / (predicted_variances + epsilon)

# Perform weighted least-squares estimation for each response variable
model_yi1 <- lm(yi1 ~ xi + x2 + x3, data = printerData, weights = weights)
model_yi2 <- lm(yi2 ~ xi + x2 + x3, data = printerData, weights = weights)
model_yi3 <- lm(yi3 ~ xi + x2 + x3, data = printerData, weights = weights)

# Plot the diagnostic plots for each model
par(mfrow=c(2,2))  # Set up the plotting layout
plot(model_yi1)
plot(model_yi2)
plot(model_yi3)
```
Here we see a much better fit in the model and are able to see that for the lower ranges of the residual values, the model is much more linear. However, at higher ranges, the model degrades and the residuals become more positive, indicating that the model is under predicting at higher values of the explanatory variables.

# 3. Question 5.21. A construction engineer studied the effect of mixing rate on the tensile strength of portland cement. From each batch she mixed, the engineer made four test samples. Of course, the mix rate was applied to the entire batch. The data follow. Perform the appropriate analysis.
```{r}

mixing_rate <- c(150, 150, 150, 150, 175, 175, 175, 175, 200, 200, 200, 200, 225, 225, 225, 225)
tensile_strength <- c(3129, 3000, 3065, 3190, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
cement_data <- data.frame(mixing_rate, tensile_strength)
model <- lmer(tensile_strength ~ mixing_rate + (1 | mixing_rate), data = cement_data)
summary(model)

```
for every unit increase in mixing rate, the expected tensile strength decreases by approximately 6.047 lb/in^2. However, it's important to note that other factors beyond mixing rate may also influence tensile strength, as indicated by the variability in tensile strength between different mixing rates. Overall, the model provides a reasonable fit to the data, but further investigation into additional factors affecting tensile strength may be necessary for a comprehensive understanding.


# 4. Question 6.12. Table B.11 contains data on the quality of Pinot Noir wine. Fit a regression model using clarity, aroma, body, flavor, and oakiness as the regressors. Investigate this model for influential observations and comment on your findings.

```{r}
wineData <- table.b11

wineFit <- lm(Quality ~ Clarity + Aroma + Body + Flavor + Oakiness, data = wineData)

plot(wineFit, which = 4)

summary(wineFit)
```
Using cooks distance, we see that observation 12, 20 and 30 are of note, being farther than everything else but have not actually crossed the dashed lines to be considered influential.

